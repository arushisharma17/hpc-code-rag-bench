\n/**************************************************************************\n**\n** Copyright (C) 1993 David E. Stewart & Zbigniew Leyk, all rights reserved.\n**\n**			     Meschach Library\n** \n** This Meschach Library is provided "as is" without any express \n** or implied warranty of any kind with respect to this software. \n** In particular the authors shall not be liable for any direct, \n** indirect, special, incidental or consequential damages arising \n** in any way from use of the software.\n** \n** Everyone is granted permission to copy, modify and redistribute this\n** Meschach Library, provided:\n**  1.  All copies contain this copyright notice.\n**  2.  All modified copies shall carry a notice stating who\n**      made the last modification and the date of such modification.\n**  3.  No charge is made for this software or works derived from it.  \n**      This clause shall not be construed as constraining other software\n**      distributed on the same medium as this software, nor is a\n**      distribution fee considered a charge.\n**\n***************************************************************************/\n\n\n/* itersym.c 17/09/93 */\n\n\n/* \n  ITERATIVE METHODS - implementation of several iterative methods;\n  see also iter0.c\n  */\n\n#include        <stdio.h>\n#include	<math.h>\n#include        "matrix.h"\n#include        "matrix2.h"\n#include	"sparse.h"\n#include        "iter.h"\n\nstatic char rcsid[] = "$Id: itersym.c,v 1.2 1995/01/30 14:55:54 des Exp $";\n\n\n#ifdef ANSI_C\nVEC	*spCHsolve(const SPMAT *,VEC *,VEC *);\nVEC	*trieig(VEC *,VEC *,MAT *);\n#else\nVEC	*spCHsolve();\nVEC	*trieig();\n#endif\n\n\n\n/* iter_spcg -- a simple interface to iter_cg() which uses sparse matrix\n   data structures\n   -- assumes that LLT contains the Cholesky factorisation of the\n   actual preconditioner;\n   use always as follows:\n   x = iter_spcg(A,LLT,b,eps,x,limit,steps);\n   or \n   x = iter_spcg(A,LLT,b,eps,VNULL,limit,steps);\n   In the second case the solution vector is created.\n   */\n#ifndef ANSI_C\nVEC  *iter_spcg(A,LLT,b,eps,x,limit,steps)\nSPMAT	*A, *LLT;\nVEC	*b, *x;\ndouble	eps;\nint *steps, limit;\n#else\nVEC  *iter_spcg(SPMAT *A, SPMAT *LLT, VEC *b, double eps, VEC *x,\n		int limit, int *steps)\n#endif\n{	\n   ITER *ip;\n   \n   ip = iter_get(0,0);\n   ip->Ax = (Fun_Ax) sp_mv_mlt;\n   ip->A_par = (void *)A;\n   ip->Bx = (Fun_Ax) spCHsolve;\n   ip->B_par = (void *)LLT;\n   ip->info = (Fun_info) NULL;\n   ip->b = b;\n   ip->eps = eps;\n   ip->limit = limit;\n   ip->x = x;\n   iter_cg(ip);\n   x = ip->x;\n   if (steps) *steps = ip->steps;\n   ip->shared_x = ip->shared_b = TRUE;\n   iter_free(ip);   /* release only ITER structure */\n   return x;		\n}\n\n/* \n  Conjugate gradients method;\n  */\n#ifndef ANSI_C\nVEC  *iter_cg(ip)\nITER *ip;\n#else\nVEC  *iter_cg(ITER *ip)\n#endif\n{\n   STATIC VEC *r = VNULL, *p = VNULL, *q = VNULL, *z = VNULL;\n   Real	alpha, beta, inner, old_inner, nres;\n   VEC *rr;   /* rr == r or rr == z */\n   \n   if (ip == INULL)\n     error(E_NULL,"iter_cg");\n   if (!ip->Ax || !ip->b)\n     error(E_NULL,"iter_cg");\n   if ( ip->x == ip->b )\n     error(E_INSITU,"iter_cg");\n   if (!ip->stop_crit)\n     error(E_NULL,"iter_cg");\n   \n   if ( ip->eps <= 0.0 )\n     ip->eps = MACHEPS;\n   \n   r = v_resize(r,ip->b->dim);\n   p = v_resize(p,ip->b->dim);\n   q = v_resize(q,ip->b->dim);\n   \n   MEM_STAT_REG(r,TYPE_VEC);\n   MEM_STAT_REG(p,TYPE_VEC);\n   MEM_STAT_REG(q,TYPE_VEC);\n   \n   if (ip->Bx != (Fun_Ax)NULL) {\n      z = v_resize(z,ip->b->dim);\n      MEM_STAT_REG(z,TYPE_VEC);\n      rr = z;\n   }\n   else rr = r;\n   \n   if (ip->x != VNULL) {\n      if (ip->x->dim != ip->b->dim)\n	error(E_SIZES,"iter_cg");\n      ip->Ax(ip->A_par,ip->x,p);    		/* p = A*x */\n      v_sub(ip->b,p,r);		 		/* r = b - A*x */\n   }\n   else {  /* ip->x == 0 */\n      ip->x = v_get(ip->b->dim);\n      ip->shared_x = FALSE;\n      v_copy(ip->b,r);\n   }\n   \n   old_inner = 0.0;\n   for ( ip->steps = 0; ip->steps <= ip->limit; ip->steps++ )\n   {\n      if ( ip->Bx )\n	(ip->Bx)(ip->B_par,r,rr);		/* rr = B*r */\n      \n      inner = in_prod(rr,r);\n      nres = sqrt(fabs(inner));\n      if (ip->info) ip->info(ip,nres,r,rr);\n      if (ip->steps == 0) ip->init_res = nres;\n      if ( ip->stop_crit(ip,nres,r,rr) ) break;\n      \n      if ( ip->steps )	/* if ( ip->steps > 0 ) ... */\n      {\n	 beta = inner/old_inner;\n	 p = v_mltadd(rr,p,beta,p);\n      }\n      else		/* if ( ip->steps == 0 ) ... */\n      {\n	 beta = 0.0;\n	 p = v_copy(rr,p);\n	 old_inner = 0.0;\n      }\n      (ip->Ax)(ip->A_par,p,q);     /* q = A*p */\n      alpha = in_prod(p,q);\n      if (sqrt(fabs(alpha)) <= MACHEPS*ip->init_res) \n	error(E_BREAKDOWN,"iter_cg");\n      alpha = inner/alpha;\n      v_mltadd(ip->x,p,alpha,ip->x);\n      v_mltadd(r,q,-alpha,r);\n      old_inner = inner;\n   }\n\n#ifdef	THREADSAFE\n   V_FREE(r);   V_FREE(p);   V_FREE(q);   V_FREE(z);\n#endif\n\n   return ip->x;\n}\n\n\n\n/* iter_lanczos -- raw lanczos algorithm -- no re-orthogonalisation\n   -- creates T matrix of size == m,\n   but no larger than before beta_k == 0\n   -- uses passed routine to do matrix-vector multiplies */\n#ifndef ANSI_C\nvoid	iter_lanczos(ip,a,b,beta2,Q)\nITER    *ip;\nVEC	*a, *b;\nReal	*beta2;\nMAT	*Q;\n#else\nvoid	iter_lanczos(ITER *ip, VEC *a, VEC *b, Real *beta2, MAT *Q)\n#endif\n{\n   int	j;\n   STATIC VEC	*v = VNULL, *w = VNULL, *tmp = VNULL;\n   Real	alpha, beta, c;\n   \n   if ( ! ip )\n     error(E_NULL,"iter_lanczos");\n   if ( ! ip->Ax || ! ip->x || ! a || ! b )\n     error(E_NULL,"iter_lanczos");\n   if ( ip->k <= 0 )\n     error(E_BOUNDS,"iter_lanczos");\n   if ( Q && ( Q->n < ip->x->dim || Q->m < ip->k ) )\n     error(E_SIZES,"iter_lanczos");\n   \n   a = v_resize(a,(unsigned int)ip->k);	\n   b = v_resize(b,(unsigned int)(ip->k-1));\n   v = v_resize(v,ip->x->dim);\n   w = v_resize(w,ip->x->dim);\n   tmp = v_resize(tmp,ip->x->dim);\n   MEM_STAT_REG(v,TYPE_VEC);\n   MEM_STAT_REG(w,TYPE_VEC);\n   MEM_STAT_REG(tmp,TYPE_VEC);\n   \n   beta = 1.0;\n   v_zero(a);\n   v_zero(b);\n   if (Q) m_zero(Q);\n   \n   /* normalise x as w */\n   c = v_norm2(ip->x);\n   if (c <= MACHEPS) { /* ip->x == 0 */\n      *beta2 = 0.0;\n      return;\n   }\n   else \n     sv_mlt(1.0/c,ip->x,w);\n   \n   (ip->Ax)(ip->A_par,w,v);\n   \n   for ( j = 0; j < ip->k; j++ )\n   {\n      /* store w in Q if Q not NULL */\n      if ( Q ) set_row(Q,j,w);\n      \n      alpha = in_prod(w,v);\n      a->ve[j] = alpha;\n      v_mltadd(v,w,-alpha,v);\n      beta = v_norm2(v);\n      if ( beta == 0.0 )\n      {\n	 *beta2 = 0.0;\n	 return;\n      }\n      \n      if ( j < ip->k-1 )\n	b->ve[j] = beta;\n      v_copy(w,tmp);\n      sv_mlt(1/beta,v,w);\n      sv_mlt(-beta,tmp,v);\n      (ip->Ax)(ip->A_par,w,tmp);\n      v_add(v,tmp,v);\n   }\n   *beta2 = beta;\n\n#ifdef	THREADSAFE\n   V_FREE(v);   V_FREE(w);   V_FREE(tmp);\n#endif\n}\n\n/* iter_splanczos -- version that uses sparse matrix data structure */\n#ifndef ANSI_C\nvoid    iter_splanczos(A,m,x0,a,b,beta2,Q)\nSPMAT	*A;\nint     m;\nVEC     *x0, *a, *b;\nReal    *beta2;\nMAT     *Q;\n#else\nvoid    iter_splanczos(SPMAT *A, int m, VEC *x0, \n		       VEC *a, VEC *b, Real *beta2, MAT *Q)\n#endif\n{	\n   ITER *ip;\n   \n   ip = iter_get(0,0);\n   ip->shared_x = ip->shared_b = TRUE;\n   ip->Ax = (Fun_Ax) sp_mv_mlt;\n   ip->A_par = (void *) A;\n   ip->x = x0;\n   ip->k = m;\n   iter_lanczos(ip,a,b,beta2,Q);	\n   iter_free(ip);   /* release only ITER structure */\n}\n\n\n#ifndef ANSI_C\nextern	double	frexp(), ldexp();\n#else\nextern	double	frexp(double num, int *exponent),\n  ldexp(double num, int exponent);\n#endif\n\n/* product -- returns the product of a long list of numbers\n   -- answer stored in mant (mantissa) and expt (exponent) */\n#ifndef ANSI_C\nstatic	double	product(a,offset,expt)\nVEC	*a;\ndouble	offset;\nint	*expt;\n#else\nstatic	double	product(VEC *a, double offset, int *expt)\n#endif\n{\n   Real	mant, tmp_fctr;\n   int	i, tmp_expt;\n   \n   if ( ! a )\n     error(E_NULL,"product");\n   \n   mant = 1.0;\n   *expt = 0;\n   if ( offset == 0.0 )\n     for ( i = 0; i < a->dim; i++ )\n     {\n	mant *= frexp(a->ve[i],&tmp_expt);\n	*expt += tmp_expt;\n	if ( ! (i % 10) )\n	{\n	   mant = frexp(mant,&tmp_expt);\n	   *expt += tmp_expt;\n	}\n     }\n   else\n     for ( i = 0; i < a->dim; i++ )\n     {\n	tmp_fctr = a->ve[i] - offset;\n	tmp_fctr += (tmp_fctr > 0.0 ) ? -MACHEPS*offset :\n	  MACHEPS*offset;\n	mant *= frexp(tmp_fctr,&tmp_expt);\n	*expt += tmp_expt;\n	if ( ! (i % 10) )\n	{\n	   mant = frexp(mant,&tmp_expt);\n	   *expt += tmp_expt;\n	}\n     }\n   \n   mant = frexp(mant,&tmp_expt);\n   *expt += tmp_expt;\n   \n   return mant;\n}\n\n/* product2 -- returns the product of a long list of numbers (except the k'th)\n   -- answer stored in mant (mantissa) and expt (exponent) */\n#ifndef ANSI_C\nstatic	double	product2(a,k,expt)\nVEC	*a;\nint	k;	/* entry of a to leave out */\nint	*expt;\n#else\nstatic	double	product2(VEC *a, int k, int *expt)\n#endif\n{\n   Real	mant, mu, tmp_fctr;\n   int	i, tmp_expt;\n   \n   if ( ! a )\n     error(E_NULL,"product2");\n   if ( k < 0 || k >= a->dim )\n     error(E_BOUNDS,"product2");\n   \n   mant = 1.0;\n   *expt = 0;\n   mu = a->ve[k];\n   for ( i = 0; i < a->dim; i++ )\n   {\n      if ( i == k )\n	continue;\n      tmp_fctr = a->ve[i] - mu;\n      tmp_fctr += ( tmp_fctr > 0.0 ) ? -MACHEPS*mu : MACHEPS*mu;\n      mant *= frexp(tmp_fctr,&tmp_expt);\n      *expt += tmp_expt;\n      if ( ! (i % 10) )\n      {\n	 mant = frexp(mant,&tmp_expt);\n	 *expt += tmp_expt;\n      }\n   }\n   mant = frexp(mant,&tmp_expt);\n   *expt += tmp_expt;\n   \n   return mant;\n}\n\n/* dbl_cmp -- comparison function to pass to qsort() */\n#ifndef ANSI_C\nstatic	int	dbl_cmp(x,y)\nReal	*x, *y;\n#else\nstatic	int	dbl_cmp(Real *x, Real *y)\n#endif\n{\n   Real	tmp;\n   \n   tmp = *x - *y;\n   return (tmp > 0 ? 1 : tmp < 0 ? -1: 0);\n}\n\n/* iter_lanczos2 -- lanczos + error estimate for every e-val\n   -- uses Cullum & Willoughby approach, Sparse Matrix Proc. 1978\n   -- returns multiple e-vals where multiple e-vals may not exist\n   -- returns evals vector */\n#ifndef ANSI_C\nVEC	*iter_lanczos2(ip,evals,err_est)\nITER 	*ip;            /* ITER structure */\nVEC	*evals;		/* eigenvalue vector */\nVEC	*err_est;	/* error estimates of eigenvalues */\n#else\nVEC	*iter_lanczos2(ITER *ip, VEC *evals, VEC *err_est)\n#endif\n{\n   VEC		*a;\n   STATIC	VEC	*b=VNULL, *a2=VNULL, *b2=VNULL;\n   Real	beta, pb_mant, det_mant, det_mant1, det_mant2;\n   int	i, pb_expt, det_expt, det_expt1, det_expt2;\n   \n   if ( ! ip )\n     error(E_NULL,"iter_lanczos2");\n   if ( ! ip->Ax || ! ip->x )\n     error(E_NULL,"iter_lanczos2");\n   if ( ip->k <= 0 )\n     error(E_RANGE,"iter_lanczos2");\n   \n   a = evals;\n   a = v_resize(a,(unsigned int)ip->k);\n   b = v_resize(b,(unsigned int)(ip->k-1));\n   MEM_STAT_REG(b,TYPE_VEC);\n   \n   iter_lanczos(ip,a,b,&beta,MNULL);\n   \n   /* printf("# beta =%g\n",beta); */\n   pb_mant = 0.0;\n   if ( err_est )\n   {\n      pb_mant = product(b,(double)0.0,&pb_expt);\n      /* printf("# pb_mant = %g, pb_expt = %d\n",pb_mant, pb_expt); */\n   }\n   \n   /* printf("# diags =\n");	v_output(a); */\n   /* printf("# off diags =\n");	v_output(b); */\n   a2 = v_resize(a2,a->dim - 1);\n   b2 = v_resize(b2,b->dim - 1);\n   MEM_STAT_REG(a2,TYPE_VEC);\n   MEM_STAT_REG(b2,TYPE_VEC);\n   for ( i = 0; i < a2->dim - 1; i++ )\n   {\n      a2->ve[i] = a->ve[i+1];\n      b2->ve[i] = b->ve[i+1];\n   }\n   a2->ve[a2->dim-1] = a->ve[a2->dim];\n   \n   trieig(a,b,MNULL);\n   \n   /* sort evals as a courtesy */\n   qsort((void *)(a->ve),(int)(a->dim),sizeof(Real),(int (*)())dbl_cmp);\n   \n   /* error estimates */\n   if ( err_est )\n   {\n      err_est = v_resize(err_est,(unsigned int)ip->k);\n      \n      trieig(a2,b2,MNULL);\n      /* printf("# a =\n");	v_output(a); */\n      /* printf("# a2 =\n");	v_output(a2); */\n      \n      for ( i = 0; i < a->dim; i++ )\n      {\n	 det_mant1 = product2(a,i,&det_expt1);\n	 det_mant2 = product(a2,(double)a->ve[i],&det_expt2);\n	 /* printf("# det_mant1=%g, det_expt1=%d\n",\n	    det_mant1,det_expt1); */\n	 /* printf("# det_mant2=%g, det_expt2=%d\n",\n	    det_mant2,det_expt2); */\n	 if ( det_mant1 == 0.0 )\n	 {   /* multiple e-val of T */\n	    err_est->ve[i] = 0.0;\n	    continue;\n	 }\n	 else if ( det_mant2 == 0.0 )\n	 {\n	    err_est->ve[i] = HUGE_VAL;\n	    continue;\n	 }\n	 if ( (det_expt1 + det_expt2) % 2 )\n	   /* if odd... */\n	   det_mant = sqrt(2.0*fabs(det_mant1*det_mant2));\n	 else /* if even... */\n	   det_mant = sqrt(fabs(det_mant1*det_mant2));\n	 det_expt = (det_expt1+det_expt2)/2;\n	 err_est->ve[i] = fabs(beta*\n			       ldexp(pb_mant/det_mant,pb_expt-det_expt));\n      }\n   }\n\n#ifdef	THREADSAFE\n   V_FREE(b);   V_FREE(a2);   V_FREE(b2);\n#endif\n\n   return a;\n}\n\n/* iter_splanczos2 -- version of iter_lanczos2() that uses sparse matrix data\n   structure */\n#ifndef ANSI_C\nVEC    *iter_splanczos2(A,m,x0,evals,err_est)\nSPMAT	*A;\nint	 m;\nVEC	*x0;		/* initial vector */\nVEC	*evals;		/* eigenvalue vector */\nVEC	*err_est;	/* error estimates of eigenvalues */\n#else\nVEC    *iter_splanczos2(SPMAT *A, int m, VEC *x0, VEC *evals, VEC *err_est)\n#endif\n{	\n   ITER *ip;\n   VEC *a;\n   \n   ip = iter_get(0,0);\n   ip->Ax = (Fun_Ax) sp_mv_mlt;\n   ip->A_par = (void *) A;\n   ip->x = x0;\n   ip->k = m;\n   a = iter_lanczos2(ip,evals,err_est);	\n   ip->shared_x = ip->shared_b = TRUE;\n   iter_free(ip);   /* release only ITER structure */\n   return a;\n}\n\n\n\n\n/*\n  Conjugate gradient method\n  Another variant - mainly for testing\n  */\n#ifndef ANSI_C\nVEC  *iter_cg1(ip)\nITER *ip;\n#else\nVEC  *iter_cg1(ITER *ip)\n#endif\n{\n   STATIC VEC *r = VNULL, *p = VNULL, *q = VNULL, *z = VNULL;\n   Real	alpha;\n   double inner,nres;\n   VEC *rr;   /* rr == r or rr == z */\n   \n   if (ip == INULL)\n     error(E_NULL,"iter_cg");\n   if (!ip->Ax || !ip->b)\n     error(E_NULL,"iter_cg");\n   if ( ip->x == ip->b )\n     error(E_INSITU,"iter_cg");\n   if (!ip->stop_crit)\n     error(E_NULL,"iter_cg");\n   \n   if ( ip->eps <= 0.0 )\n     ip->eps = MACHEPS;\n   \n   r = v_resize(r,ip->b->dim);\n   p = v_resize(p,ip->b->dim);\n   q = v_resize(q,ip->b->dim);\n   \n   MEM_STAT_REG(r,TYPE_VEC);\n   MEM_STAT_REG(p,TYPE_VEC);\n   MEM_STAT_REG(q,TYPE_VEC);\n   \n   if (ip->Bx != (Fun_Ax)NULL) {\n      z = v_resize(z,ip->b->dim);\n      MEM_STAT_REG(z,TYPE_VEC);\n      rr = z;\n   }\n   else rr = r;\n   \n   if (ip->x != VNULL) {\n      if (ip->x->dim != ip->b->dim)\n	error(E_SIZES,"iter_cg");\n      ip->Ax(ip->A_par,ip->x,p);    		/* p = A*x */\n      v_sub(ip->b,p,r);		 		/* r = b - A*x */\n   }\n   else {  /* ip->x == 0 */\n      ip->x = v_get(ip->b->dim);\n      ip->shared_x = FALSE;\n      v_copy(ip->b,r);\n   }\n   \n   if (ip->Bx) (ip->Bx)(ip->B_par,r,p);\n   else v_copy(r,p);\n   \n   inner = in_prod(p,r);\n   nres = sqrt(fabs(inner));\n   if (ip->info) ip->info(ip,nres,r,p);\n   if ( nres == 0.0) return ip->x;\n   \n   for ( ip->steps = 0; ip->steps <= ip->limit; ip->steps++ )\n   {\n      ip->Ax(ip->A_par,p,q);\n      inner = in_prod(q,p);\n      if (sqrt(fabs(inner)) <= MACHEPS*ip->init_res)\n	error(E_BREAKDOWN,"iter_cg1");\n\n      alpha = in_prod(p,r)/inner;\n      v_mltadd(ip->x,p,alpha,ip->x);\n      v_mltadd(r,q,-alpha,r);\n      \n      rr = r;\n      if (ip->Bx) {\n	 ip->Bx(ip->B_par,r,z);\n	 rr = z;\n      }\n      \n      nres = in_prod(r,rr);\n      if (nres < 0.0) {\n	 warning(WARN_RES_LESS_0,"iter_cg");\n	 break;\n      }\n      nres = sqrt(fabs(nres));\n      if (ip->info) ip->info(ip,nres,r,z);\n      if (ip->steps == 0) ip->init_res = nres;\n      if ( ip->stop_crit(ip,nres,r,z) ) break;\n      \n      alpha = -in_prod(rr,q)/inner;\n      v_mltadd(rr,p,alpha,p);\n      \n   }\n\n#ifdef	THREADSAFE\n   V_FREE(r);   V_FREE(p);   V_FREE(q);   V_FREE(z);\n#endif\n\n   return ip->x;\n}\n\n\n